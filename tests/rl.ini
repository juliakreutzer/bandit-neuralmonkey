[main]
name="translation with RL training"
tf_manager=<tf_manager>
output="tests/outputs/rl"
overwrite_output_dir=True
batch_size=10
epochs=3
train_dataset=<train_data>
train_buffer=<train_buffer>
buffer_trainer=<buffer_trainer>
buffer_freq=20
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>]
postprocess=None
evaluation=[("target", evaluators.BLEU)]
logging_period=20
validation_period=60
runners_batch_size=2
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=4
num_sessions=1

[train_data]
class=dataset.load_dataset_from_files
s_source="tests/data/train.tc.en"
s_target="tests/data/train.tc.de"
lazy=True

[train_buffer]
class=dataset.buffer.TrainingBuffer
max_size=10
log_file="buffer.log"

[val_data]
class=dataset.load_dataset_from_files
s_source="tests/data/val.tc.en"
s_target="tests/data/val.tc.de"

[encoder_vocabulary]
class=vocabulary.from_wordlist
path="tests/outputs/vocab/encoder_vocab.tsv"

[encoder]
class=encoders.recurrent.SentenceEncoder
name="sentence_encoder"
rnn_size=7
max_input_len=10
embedding_size=11
dropout_keep_prob=0.5
data_id="source"
vocabulary=<encoder_vocabulary>

[attention]
class=attention.Attention
name="attention_sentence_encoder"
encoder=<encoder>

[decoder_vocabulary]
class=vocabulary.from_wordlist
path="tests/outputs/vocab/decoder_vocab.tsv"

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<encoder>]
rnn_size=8
embedding_size=9
attentions=[<attention>]
dropout_keep_prob=0.5
data_id="target"
max_output_len=10
vocabulary=<decoder_vocabulary>

[rl]
class=trainers.rl_trainer.rl_objective
decoder=<decoder>
src_vocabulary=<encoder_vocabulary>
trg_vocabulary=<decoder_vocabulary>
buffer=<train_buffer>
reward_function=<reward>
subtract_baseline=True
normalize=False
sample_size=1
ce_smoothing=0.0
entropy_regularization=0.0
greedy=False

[wxent]
class=trainers.rl_trainer.wxent_objective
decoder=<decoder>

[reward]
class=evaluators.gleu.GLEUEvaluator
name="GLEU"

[trainer]
class=trainers.generic_trainer.GenericTrainer
objectives=[<rl>]
l2_weight=1.0e-8
clip_norm=1.0
optimizer=<adam>

[buffer_trainer]
class=trainers.generic_trainer.GenericTrainer
objectives=[<wxent>]
l2_weight=1.0e-8
clip_norm=1.0
optimizer=<adam>

[buffer_adam]
class=tf.train.AdamOptimizer
beta1=0.9
beta2=0.98
epsilon=1.0e-9
learning_rate=0.2

[adam]
class=tf.train.AdamOptimizer
beta1=0.9
beta2=0.98
epsilon=1.0e-9
learning_rate=0.2

[runner]
class=runners.GreedyRunner
decoder=<decoder>
output_series="target"
